{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "R.version.string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PvvWwMz-5Q_K",
        "outputId": "6e47fa24-df26-4705-d925-14d0102d9232"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'R version 4.4.2 (2024-10-31)'"
            ],
            "text/markdown": "'R version 4.4.2 (2024-10-31)'",
            "text/latex": "'R version 4.4.2 (2024-10-31)'",
            "text/plain": [
              "[1] \"R version 4.4.2 (2024-10-31)\""
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_u6Dyj1xehk"
      },
      "source": [
        "# Part 1 Regression (50 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "marked-instrument"
      },
      "source": [
        "Around 1000 people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual. You need to build regression models to optimally predict the variable in the survey dataset called 'happiness' based on any, or all, of the other survey question responses.\n",
        "\n",
        "You have been provided with two datasets, ```regression_train.csv``` and ```regression_test.csv```. Using these datasets, you hope to build a model that can predict happiness level using the other variables. ```regression_train.csv``` comes with the ground-truth target label (i.e. happiness level) whereas `regression_test.csv` comes with independent variables (input information) only.\n",
        "\n",
        "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict happiness. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. E.g. the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Gbtyt1g7iv"
      },
      "source": [
        "**PLEASE NOTE THAT THE USE OF LIBRARIES ARE PROHIBITED IN THESE QUESTIONS UNLESS STATED OTHERWISE, ANSWERS USING LIBRARIES WILL RECEIVE 0 MARKS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wound-marriage"
      },
      "source": [
        "## Question 1 (NO LIBRARIES ALLOWED) (4 Mark)\n",
        "Please load the ```regression_train.csv``` and fit a [$\\textbf{multiple linear regression model}$](https://en.wikipedia.org/wiki/Linear_regression) with 'happiness' being the target variable. According to the summary table, which predictors do you think are possibly associated with the target variable (use the significance level of 0.01), and which are the **Top 5** strongest predictors? Please write an R script to automatically fetch and print this information.\n",
        "\n",
        "**NOTE**: Manually doing the above tasks will result in 0 marks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tidyverse\")\n",
        "\n",
        "library(tidyverse)\n"
      ],
      "metadata": {
        "id": "EEB-O8LeaM-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82844513-9666-4315-97f0-8016b5caa43a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
            "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
            "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
            "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
            "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data <- read.csv(\"/content/regression_train.csv\")\n",
        "\n",
        "# Fit the linear regression model\n",
        "model <- lm(happiness ~ ., data = train_data)\n",
        "\n",
        "# Get p values from the model\n",
        "p_values <- summary(model)$coefficients[, \"Pr(>|t|)\"]\n",
        "\n",
        "# Filter predictors with p-values <= 0.01\n",
        "significant_predictors <- names(p_values[p_values <= 0.01])\n",
        "\n",
        "# Sort predictors by p-value and select the top 5\n",
        "top_5_predictors <- names(p_values[order(p_values)][1:5])\n",
        "\n",
        "# Print the top 5 strongest predictors\n",
        "cat(\"\\nTop 5 Strongest Predictors:\\n\")\n",
        "print(top_5_predictors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX6j6kTw5ifV",
        "outputId": "ad6b6e33-7ad3-46d6-b9f8-6b5144e249e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Strongest Predictors:\n",
            "[1] \"income80k - 120k\" \"income50k - 80k\"  \"income200k above\" \"income20k - 50k\" \n",
            "[5] \"income15k - 20k\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "royal-finland"
      },
      "source": [
        "## Question 2 (2 Mark)\n",
        "[**R squared**](https://en.wikipedia.org/wiki/Coefficient_of_determination) from the summary table reflects that the full model doesn't fit the training dataset well; thus, you try to quantify the error between the values of the ground-truth and those of the model prediction. You want to write a function to predict 'happiness' with the given dataset and calculate the [root mean squared error (rMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the model predictions and the ground truths. Please test this function on the full model and the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCRj5S-H0WOy",
        "outputId": "1f37cd3e-7621-4390-e14b-40e7777ff286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on the training dataset for the full model:  6.771948 \n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_data <- read.csv(\"/content/regression_train.csv\")\n",
        "\n",
        "# Perform a full model for multiple linear regression\n",
        "model <- lm(happiness ~ ., data = train_data)\n",
        "\n",
        "calculate_rmse <- function(model, data) {\n",
        "  # Predict 'happiness' values using the model\n",
        "  predictions <- predict(model, newdata = data)\n",
        "\n",
        "  # Calculate RMSE\n",
        "  rmse <- sqrt(mean((data$happiness - predictions)^2))\n",
        "  # return rmse\n",
        "  return(rmse)\n",
        "}\n",
        "\n",
        "# Call function calculate rmse\n",
        "full_model_rmse <- calculate_rmse(model, train_data)\n",
        "\n",
        "cat(\"Root Mean Squared Error (RMSE) on the training dataset for the full model: \", full_model_rmse, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eight-newman"
      },
      "source": [
        "## Question 3 (2 Marks)\n",
        "You find the full model complicated and try to reduce the complexity by performing [bidirectional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) with [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
        "\n",
        "Calculate the **rMSE** of this new model with the function that you implemented previously. Is there anything you find unusual? Explain your findings in 100 words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data <- read.csv(\"/content/regression_train.csv\")\n",
        "\n",
        "# Set Full Model\n",
        "fullmod <- lm(happiness ~ ., train_data)\n",
        "\n",
        "# Perform BIC\n",
        "bic= step(fullmod, trace = 0, k = log(nrow(train_data)), direction = \"both\")\n",
        "\n",
        "# Make predictions using the bic model\n",
        "predicted_values <- predict(bic, newdata = train_data)\n",
        "\n",
        "# Calculate the residuals\n",
        "residuals <- train_data$happiness - predicted_values\n",
        "\n",
        "# Calculate the mean squared error (MSE)\n",
        "mse <- mean(residuals^2)\n",
        "\n",
        "# Calculate the root mean squared error (RMSE)\n",
        "rmse <- sqrt(mse)\n",
        "\n",
        "# Print the RMSE\n",
        "cat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0uvAiNlzGG1",
        "outputId": "53a72542-bd54-4935-907c-a47c9441d139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 7.461174 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "The RMSE for the full model is 6.77. When we simplified the model using BIC, the RMSE increased slightly to 7.46, indicating that the simplified model's predictive performance on the training data was slightly worse. The unusual outcome suggests that the full model, including all predictors, may reveal important correlations, making it more effective. The result contradicts the popular idea that fewer predictors lead to a better model. However, to ensure the full model is genuinely better,  we must test it on an independent dataset to guarantee its generalization and predictive accuracy beyond the training data.\n"
      ],
      "metadata": {
        "id": "90kIXzG-tiri"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latter-translation"
      },
      "source": [
        "## Question 4 (2 Mark)\n",
        "Although stepwise regression has reduced the model complexity significantly, the model still contains a lot of variables that we want to remove. Therefore, you are interested in lightweight linear regression models with ONLY TWO predictors. Write a script to automatically find the best lightweight model which corresponds to the model with the least **rMSE** on the training dataset. Compare the **rMSE** of the best lightweight model with the **rMSE** of the full model - ```lm.fit``` - that you built previously. Give an explanation for these results based on consideration of the predictors involved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCEvwU8V0WO0",
        "outputId": "bbcbe785-f60f-47f7-a763-64f1a12e3657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for the best lightweight model:  7.948134 \n",
            "RMSE for the full model:  6.771948 \n",
            "The best two predictors for the lightweight model are:  income  and  alwaysStressed \n",
            "The full model performs better in terms of RMSE, suggesting that including more predictors provides a better fit to the data.\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate RMSE\n",
        "calculate_rmse <- function(model, data) {\n",
        "  predictions <- predict(model, data)\n",
        "  rmse <- sqrt(mean((data$happiness - predictions)^2))\n",
        "  return(rmse)\n",
        "}\n",
        "\n",
        "# Load data\n",
        "data <- read.csv(\"/content/regression_train.csv\")\n",
        "\n",
        "# Initialize variables\n",
        "best_model <- NULL\n",
        "best_rmse <- Inf\n",
        "best_predictors <- NULL\n",
        "\n",
        "# Get the list of predictor\n",
        "predictors <- colnames(data)[!colnames(data) %in% \"happiness\"]\n",
        "\n",
        "# Iterate through all combinations of two predictors\n",
        "for (i in 1:(length(predictors) - 1)) {\n",
        "  for (j in (i + 1):length(predictors)) {\n",
        "    # Get the two predictors for this iteration\n",
        "    selected_predictors <- c(predictors[i], predictors[j])\n",
        "\n",
        "    # Create the formula for the model\n",
        "    formula <- as.formula(paste(\"happiness ~\", paste(selected_predictors, collapse = \" + \")))\n",
        "\n",
        "    # Fit the model with two predictors\n",
        "    model <- lm(formula, data)\n",
        "\n",
        "    # Calculate the RMSE for this model\n",
        "    rmse <- calculate_rmse(model, data)\n",
        "\n",
        "    # Check if this model has a lower RMSE\n",
        "    if (rmse < best_rmse) {\n",
        "      best_rmse <- rmse\n",
        "      best_model <- model\n",
        "      # Store the selected predictors\n",
        "      best_predictors <- selected_predictors\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Calculate the RMSE for the full model\n",
        "full_model_rmse <- calculate_rmse(lm(happiness ~ ., data), data)\n",
        "\n",
        "cat(\"RMSE for the best lightweight model: \", best_rmse, \"\\n\")\n",
        "cat(\"RMSE for the full model: \", full_model_rmse, \"\\n\")\n",
        "\n",
        "# Print the best two predictors\n",
        "cat(\"The best two predictors for the lightweight model are: \", best_predictors[1], \" and \", best_predictors[2], \"\\n\")\n",
        "\n",
        "# Compare the results\n",
        "if (best_rmse < full_model_rmse) {\n",
        "  cat(\"The best lightweight model has a lower RMSE, indicating better performance with just two predictors.\\n\")\n",
        "} else {\n",
        "  cat(\"The full model performs better in terms of RMSE, suggesting that including more predictors provides a better fit to the data.\\n\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4P9QL6tg7ix"
      },
      "source": [
        "### ANSWER (TEXT)\n",
        "\n",
        "Lightmodel uses only \"income\" and \"alwaysStressed\" as predictors, which simplifies the model but reduces accuracy (RMSE 7.95). In contrast, a complex model with all predictors performs better (RMSE 6.77) because it considers more factors influencing happiness. The simpler model sacrifices accuracy for simplicity by omitting important factors. However, it's essential to find the right balance in the number of predictors. Too many predictors can lead to overfitting, where the model fits the training data perfectly but needs help with new data. It is important to balance the number of predictors used in predictions to prevent overfitting and improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preceding-chicago"
      },
      "source": [
        "## Question 5 (Libraries are allowed) (40 Marks)\n",
        "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limited to the aforementioned steps in this assignment. To simulate for a realistic modelling process, this question will be in the form of a [Kaggle competition](https://www.kaggle.com/t/a83b5384ae9849848356e0c1966771ec) among students to find out who has the best model.\n",
        "\n",
        "Thus, you **will be graded** by the **rMSE** performance of your model, the better your model, the higher your score. Additionally, you need to describe/document your thought process in this model building process, this is akin to showing your working properly for the mathematic sections. If you don't clearly document the reasonings behind the model you use, we will have to make some deductions on your scores.\n",
        "\n",
        "This is the [video tutorial](https://www.youtube.com/watch?v=rkXc25Uvyl4) on how to join any Kaggle competition.\n",
        "\n",
        "When you optimize your model's performance, you can use any supervised model that you know and feature selection might be a big help as well. [Check the non-exhaustive set of R functions relevant to this unit](https://lms.monash.edu/mod/resource/view.php?id=12098821) for ideas for different models to try.\n",
        "\n",
        "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
        "\n",
        "```install.packages(\"some package\", repos='http://cran.us.r-project.org')```\n",
        "\n",
        "```library(\"some package\")```\n",
        "\n",
        "Remember that if we cannot run your code, we will have to give you a deduction. Our suggestion is for you to use the standard ```R version 3.6.1```\n",
        "\n",
        "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation :\n",
        "\n",
        "First, I used two techniques, Lasso and Boruta, to identify the predictors that truly affect happiness. I chose predictors that can either increase or decrease happiness, making sure to include both positive and negative correlations. This approach considers various factors influencing our happiness.\n",
        "\n",
        "I chose the Random Forest algorithm due to its robustness and adaptability. Random Forest is a powerful tool that can handle multiple predictors, whether they are categorical (using one-hot encoding) or numerical. It can even handle predictors that have high collinearity. One of the unique strengths of Random Forest is that it keeps the model focused on complex details that could lead to overfitting. To ensure data integrity, I set a random seed, which promotes reproducibility when training the Random Forest model.\n",
        "\n",
        "To fine-tune the model, I explored various hyperparameters, including ntree (the number of trees in the forest), mtry (the number of features considered at each split), nodesize (the minimum number of observations in terminal nodes), and minsplit (the minimum number of data points required to do a split). I also used cross-validation to evaluate the model's performance under different settings through repeated testing. I optimized my model by minimizing the Root Mean Square Error (RMSE) and quantifying the difference between predicted and actual happiness levels in training data. I compared the average RMSE, calculated as the mean of RMSE values across all cross-validation folds, with the best RMSE to ensure consistent performance. After several loops to find the best parameter, I will use the best model to predict test data.\n",
        "\n",
        "Furthermore, I used a learning curve to visualize the model's performance with the training data. It helps assess the model's tendency toward overfitting or underfitting, ensuring its ability to generalize well to unseen data."
      ],
      "metadata": {
        "id": "tQ0F31CtLoNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages('randomForest')\n",
        "install.packages('caret')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA8Hqiy5elCr",
        "outputId": "18919a09-f4f7-4d3e-ab25-f6d0c37ff34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "library(caret)\n",
        "library(randomForest)\n",
        "\n",
        "# Load training data\n",
        "train_data <- read.csv(\"/content/regression_train.csv\")\n",
        "\n",
        "# Define predictor variables\n",
        "predictor_variables <- c(\"alwaysAnxious\", \"alwaysStressed\", \"alwaysDepressed\",\n",
        "                         \"iDontHaveFunWithOtherPeople\", \"alwaysHaveFun\",\n",
        "                         \"iUsuallyHaveAGoodInfluenceOnEvents\", \"iLaughALot\",\n",
        "                         \"alwaysLoveAndCareForYourself\",\"iAlwaysHaveACheerfulEffectOnOthers\")\n",
        "\n",
        "# Convert income to factor\n",
        "train_data$income <- factor(train_data$income)\n",
        "\n",
        "# Create one-hot encoder\n",
        "dummy_encoder <- dummyVars(~ income, data = train_data)\n",
        "encoded_train_income <- predict(dummy_encoder, newdata = train_data)\n",
        "\n",
        "# Combine predictors and encoded income for training data\n",
        "X_train <- cbind(train_data[, predictor_variables], encoded_train_income)\n",
        "\n",
        "# Define the target variable\n",
        "y_train <- train_data$happiness\n",
        "\n",
        "# Train the Random Forest model\n",
        "set.seed(123)\n",
        "rf_model <- randomForest(\n",
        "  x = X_train,\n",
        "  y = y_train,\n",
        "  ntree = 500,\n",
        "  importance = TRUE\n",
        ")\n",
        "\n",
        "# Load test data\n",
        "test_data <- read.csv(\"/content/regression_test.csv\")\n",
        "\n",
        "test_data$income <- factor(test_data$income, levels = levels(train_data$income))\n",
        "\n",
        "encoded_test_income <- tryCatch({\n",
        "  predict(dummy_encoder, newdata = test_data)\n",
        "}, error = function(e) {\n",
        "  encoded_test_income <- predict(dummy_encoder, newdata = test_data, na.action = na.pass)\n",
        "  missing_levels <- setdiff(colnames(encoded_train_income), colnames(encoded_test_income))\n",
        "  for (lvl in missing_levels) {\n",
        "    encoded_test_income[, lvl] <- 0\n",
        "  }\n",
        "  encoded_test_income <- encoded_test_income[, colnames(encoded_train_income), drop = FALSE]\n",
        "  encoded_test_income\n",
        "})\n",
        "\n",
        "X_test <- cbind(test_data[, predictor_variables], encoded_test_income)\n",
        "\n",
        "pred.label <- predict(rf_model, X_test)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "write.csv(\n",
        "  data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),\n",
        "  \"RegressionPredictLabel.csv\",\n",
        "  row.names = FALSE\n",
        ")\n"
      ],
      "metadata": {
        "id": "m9vMsOTrBOvF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSE Result\n",
        "The private leaderboard is calculated with approximately **71%** of the test data.\n",
        "\n",
        "The Random Forest approach achieved an RMSE of **3.90118** on the public leaderboard and **5.47298** on the private leaderboard in Kaggle."
      ],
      "metadata": {
        "id": "3TbEci62Fqqv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_t1sKFs0WO3"
      },
      "source": [
        "# Part 2 Classification (50 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CT7BFpX0WO3"
      },
      "source": [
        "Around 1000 people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual, but this time we want to predict a categorical score for perfect mental health, rather than a continuous score. You need to build 5-class classification models to optimally predict the variable in the survey dataset called 'perfectMentalHealth' based on any, or all, of the other survey question responses.\n",
        "\n",
        "You have been provided with two datasets, ```classification_train.csv``` and ```classification_test.csv```. Using these datasets, you hope to build a model that can predict 'perfectMentalHealth' using the other variables. ```classification_train.csv``` comes with the ground-truth target label (i.e. 'perfectMentalHealth' happiness classes) whereas `classification_test.csv` comes with independent variables (input information) only.\n",
        "\n",
        "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict 'perfectMentalHealth'. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. E.g. the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'\n",
        "\n",
        "This question will also be in the form of a [Kaggle competition](https://www.kaggle.com/t/9288c3d1ae9648d88b8ff809e22d44d2) among students to find out who has the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation :\n",
        "\n",
        "The goal was to build a robust model that could predict \"perfect mental health\" at a high level of accuracy. To achieve this, I utilised a feature selection technique known as the Kruskal-Wallis test. This non-parametric statistical test is particularly suitable for evaluating the relationship between an ordinal or continuous predictor variable and a categorical target variable. The choice of the Kruskal-Wallis test was motivated by several factors:\n",
        "\n",
        "**Suitability for Ordinal Data**:\n",
        "\n",
        "All predictors in the dataset were ordinal, and Kruskal-Wallis does not assume normality of data, making it a natural fit for such predictors.\n",
        "\n",
        "**Capturing Non-linear Relationships**:\n",
        "\n",
        "Unlike correlation-based methods, Kruskal-Wallis is capable of detecting non-linear associations between predictors and the target variable.\n",
        "\n",
        "**Reduction of Dimensionality**:\n",
        "\n",
        "By filtering out predictors with insignificant relationships to the target variable (p-value ≥ 0.05), the test helped focus the model on the most meaningful features, reducing overfitting and improving interpretability.\n",
        "\n",
        "\n",
        "In the model selection process, I considered several other algorithms, including support vector machine, XGBoost, GBM, and logistic regression. After a thorough evaluation, I chose the Random Forest algorithm due to its performance and ability to handle predictors that are closely related without making the model overly complex, thus reducing the risk of overfitting. To maintain the integrity, I used a random seed for reproducibility.\n"
      ],
      "metadata": {
        "id": "MDKiJEg5X4_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"randomForest\")\n",
        "install.packages(\"caret\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUYvnSM6r4rL",
        "outputId": "2580283c-e5bb-4a94-c39b-da6bb2cd3316"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘listenv’, ‘parallelly’, ‘future’, ‘globals’, ‘shape’, ‘future.apply’, ‘numDeriv’, ‘progressr’, ‘SQUAREM’, ‘diagram’, ‘lava’, ‘prodlim’, ‘proxy’, ‘iterators’, ‘clock’, ‘gower’, ‘hardhat’, ‘ipred’, ‘timeDate’, ‘e1071’, ‘foreach’, ‘ModelMetrics’, ‘plyr’, ‘pROC’, ‘recipes’, ‘reshape2’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "library(randomForest)\n",
        "library(caret)\n",
        "\n",
        "# Load training and test data\n",
        "train_data <- read.csv(\"/content/classification_train.csv\")\n",
        "test_data <- read.csv(\"/content/classification_test.csv\")\n",
        "\n",
        "# Define all predictor variables (all columns except the target variable)\n",
        "predictors <- setdiff(names(train_data), \"perfectMentalHealth\")\n",
        "\n",
        "# Define target variable\n",
        "target_variable <- \"perfectMentalHealth\"\n",
        "\n",
        "# Convert the target variable to a factor\n",
        "train_data[, target_variable] <- as.factor(train_data[, target_variable])\n",
        "\n",
        "# Initialize an empty data frame for test results\n",
        "kw_results <- data.frame(Predictor = character(0), P_Value = numeric(0), stringsAsFactors = FALSE)\n",
        "\n",
        "# Perform Kruskal-Wallis test for each predictor\n",
        "for (predictor in predictors) {\n",
        "  if (any(is.na(train_data[[predictor]]))) next\n",
        "\n",
        "  # Perform Kruskal-Wallis test\n",
        "  kw_test <- kruskal.test(as.formula(paste(predictor, \"~\", target_variable)), data = train_data)\n",
        "  p_value <- kw_test$p.value\n",
        "\n",
        "  # Store results\n",
        "  kw_results <- rbind(kw_results, data.frame(Predictor = predictor, P_Value = p_value))\n",
        "}\n",
        "\n",
        "# Sort predictors by their p-value\n",
        "kw_results <- kw_results[order(kw_results$P_Value), ]\n",
        "print(\"Kruskal-Wallis Test Results:\")\n",
        "print(kw_results)\n",
        "\n",
        "# Filter predictors with p-value < 0.05\n",
        "significant_predictors <- kw_results$Predictor[kw_results$P_Value < 0.05]\n",
        "cat(\"Significant predictors (p-value < 0.05):\", paste(significant_predictors, collapse = \", \"), \"\\n\")\n",
        "\n",
        "# Use only significant predictors for Random Forest\n",
        "if (length(significant_predictors) > 0) {\n",
        "  predictors <- significant_predictors\n",
        "} else {\n",
        "  cat(\"No significant predictors found! Using all predictors.\\n\")\n",
        "}\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "set.seed(123)\n",
        "train_indices <- createDataPartition(train_data[[target_variable]], p = 0.8, list = FALSE)\n",
        "train_subset <- train_data[train_indices, ]\n",
        "val_subset <- train_data[-train_indices, ]\n",
        "\n",
        "# Train a Random Forest model with the selected predictors\n",
        "set.seed(123)\n",
        "rf_model <- randomForest(\n",
        "  x = train_subset[, predictors],\n",
        "  y = train_subset[[target_variable]],\n",
        "  ntree = 500,\n",
        "  mtry = 2,\n",
        "  importance = TRUE\n",
        ")\n",
        "\n",
        "# Save the trained model to an RDS file\n",
        "saveRDS(rf_model, \"rf_model_kw_selected.rds\")\n",
        "cat(\"Random Forest model saved to 'rf_model_kw_selected.rds'\\n\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions <- predict(rf_model, newdata = test_data[, predictors])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "write.csv(\n",
        "  data.frame(\"RowIndex\" = seq(1, nrow(test_data)), \"Prediction\" = test_predictions),\n",
        "  \"RandomForestClassificationPredictions.csv\",\n",
        "  row.names = FALSE\n",
        ")\n",
        "cat(\"Test predictions saved to 'RandomForestClassificationPredictions.csv'\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvIQd6MxzNTE",
        "outputId": "8a413195-3def-43fa-d74d-03dcf45a0950"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Kruskal-Wallis Test Results:\"\n",
            "                                                         Predictor      P_Value\n",
            "19                                    alwaysLoveAndCareForYourself 1.951199e-31\n",
            "40                                                      lifeIsGood 1.102252e-25\n",
            "16                                                   alwaysHaveFun 7.868241e-22\n",
            "14                                                      alwaysCalm 1.611113e-19\n",
            "27                         iAmWellSatisfiedAboutEverythingInMyLife 1.953604e-17\n",
            "33 alwaysEngageInPreparingAndUsingYourSkillsAndTalentsInOrderToGai 2.003534e-16\n",
            "37                                    iFeelThatLifeIsVeryRewarding 2.102050e-15\n",
            "12                                                  alwaysStressed 6.509653e-14\n",
            "30                      iFeelThatIAmNotEspeciallyInControlOfMyLife 2.248593e-12\n",
            "20                                     extremelyGoodAbilityToSense 6.119040e-09\n",
            "23                                      iRarelyWakeUpFeelingRested 6.979445e-09\n",
            "31                              iUsuallyHaveAGoodInfluenceOnEvents 9.580179e-09\n",
            "36                       iDontFeelParticularlyPleasedWithTheWayIAm 1.945575e-08\n",
            "38                      iHaveVeryWarmFeelingsTowardsAlmostEveryone 2.101775e-08\n",
            "10                 doYouFeelASenseOfPurposeAndMeaningInYourLife105 3.742950e-08\n",
            "29                              iAlwaysHaveACheerfulEffectOnOthers 3.826907e-08\n",
            "26                             iDoNotThinkThatTheWorldIsAGoodPlace 5.139464e-08\n",
            "28                                         iFindBeautyInSomeThings 6.248438e-08\n",
            "34                                            alwaysMakingProgress 7.088059e-08\n",
            "41                                                      iLaughALot 8.706927e-08\n",
            "24                                          iFindMostThingsAmusing 7.635419e-07\n",
            "11                                                   alwaysAnxious 1.175520e-06\n",
            "18                                                 alwaysDepressed 1.465455e-05\n",
            "35                                       extremelyGoodCommunicator 3.075759e-05\n",
            "32                                     iDontHaveFunWithOtherPeople 2.472092e-04\n",
            "13                   alwaysAccountableAndResponsibleForYourActions 1.349593e-03\n",
            "25                                   iAmAlwaysCommittedAndInvolved 3.031694e-03\n",
            "39                      iAmNotParticularlyOptimisticAboutTheFuture 5.181959e-03\n",
            "2                                                           income 5.249655e-03\n",
            "17                                                   alwaysSerious 1.059535e-02\n",
            "42                                       iDontThinkILookAttractive 1.371055e-02\n",
            "21                                     alwaysHaveDigestiveProblems 3.383980e-02\n",
            "15                               myBodyIsHypermobileAndLovesToMove 1.889115e-01\n",
            "5  howDoYouReconcileSpiritualBeliefsWithScientificOrRationalThinki 2.750379e-01\n",
            "4                  doYouFeelASenseOfPurposeAndMeaningInYourLife104 3.507899e-01\n",
            "22                             iAmIntenselyInterestedInOtherPeople 4.347109e-01\n",
            "3                      whatIsYourHeightExpressItAsANumberInMetresM 4.659437e-01\n",
            "6        howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends 4.683469e-01\n",
            "9  doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer 4.758039e-01\n",
            "8  howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV 5.253035e-01\n",
            "7      doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded 5.527594e-01\n",
            "1                                                           gender 9.753890e-01\n",
            "Significant predictors (p-value < 0.05): alwaysLoveAndCareForYourself, lifeIsGood, alwaysHaveFun, alwaysCalm, iAmWellSatisfiedAboutEverythingInMyLife, alwaysEngageInPreparingAndUsingYourSkillsAndTalentsInOrderToGai, iFeelThatLifeIsVeryRewarding, alwaysStressed, iFeelThatIAmNotEspeciallyInControlOfMyLife, extremelyGoodAbilityToSense, iRarelyWakeUpFeelingRested, iUsuallyHaveAGoodInfluenceOnEvents, iDontFeelParticularlyPleasedWithTheWayIAm, iHaveVeryWarmFeelingsTowardsAlmostEveryone, doYouFeelASenseOfPurposeAndMeaningInYourLife105, iAlwaysHaveACheerfulEffectOnOthers, iDoNotThinkThatTheWorldIsAGoodPlace, iFindBeautyInSomeThings, alwaysMakingProgress, iLaughALot, iFindMostThingsAmusing, alwaysAnxious, alwaysDepressed, extremelyGoodCommunicator, iDontHaveFunWithOtherPeople, alwaysAccountableAndResponsibleForYourActions, iAmAlwaysCommittedAndInvolved, iAmNotParticularlyOptimisticAboutTheFuture, income, alwaysSerious, iDontThinkILookAttractive, alwaysHaveDigestiveProblems \n",
            "Random Forest model saved to 'rf_model_kw_selected.rds'\n",
            "Test predictions saved to 'RandomForestClassificationPredictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During my initial approach, I struggled to achieve higher leaderboard rankings.\n",
        "\n",
        "However, by revisiting this problem and applying the refined methodology outlined here, I was able to achieve:\n",
        "\n",
        "Private leaderboard score (71% of the data): **0.40407**,\n",
        "corresponding to Rank **3/167** on the private leaderboard.\n",
        "\n",
        "\n",
        "Public leaderboard score (29% of the data): **0.66246**, corresponding to Rank **65/167** on the public leaderboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vror0vVH4zft"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}